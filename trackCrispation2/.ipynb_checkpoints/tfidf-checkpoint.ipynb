{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "french_stopwords = list(fr_stop)\n",
    "from scipy import stats\n",
    "import math\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadJsonFile(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        DicConfig = json.load(f)\n",
    "    return DicConfig\n",
    "\n",
    "\n",
    "def GlobalDicDeplier(OneDic):\n",
    "    for k,v in OneDic.items():\n",
    "        exec('globals()[k] = v')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Config variables\n"
     ]
    }
   ],
   "source": [
    "DicConfig = LoadJsonFile(os.path.join(os.getcwd(),\"config.json\"))\n",
    "GlobalDicDeplier(DicConfig)\n",
    "sys.path.append(Root)\n",
    "from fun import *\n",
    "\n",
    "print(\"Load Config variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reffam = PickleLoad(os.path.join(Root,FolderProject,\"RefFam.pkl\"))\n",
    "refrt = PickleLoad(os.path.join(Root,FolderProject,\"RefRT.pkl\"))\n",
    "TweetsStats = refrt.groupby(\"AUTHORTWEETID\").size().reset_index().rename(columns = {0:\"w\"})\n",
    "TweetsToAnalyse = reffam[[\"AUTHORTWEETID\",\"AUTHORTWEETCONTENT\",\"AUTHORTWEETUNIXEPOCH\"]].\\\n",
    "rename(columns = {\"AUTHORTWEETID\":\"TWEETID\",\"AUTHORTWEETCONTENT\":\"TWEETCONTENT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute tfidf score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(RemoveWordsPeriod,TooFrequentThreshold,TooInfrequentThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data!\n"
     ]
    }
   ],
   "source": [
    "corpus.LoadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.ProcessCorpus(TweetsToAnalyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TooInfrequentThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 3\n",
      "2 / 3\n",
      "3 / 3\n"
     ]
    }
   ],
   "source": [
    "corpus.ComputeCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.SaveOnDisk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = corpus.DocsRepresentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add some informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D</th>\n",
       "      <th>Rank</th>\n",
       "      <th>f</th>\n",
       "      <th>idf</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>word</th>\n",
       "      <th>AUTHORTWEETID</th>\n",
       "      <th>w</th>\n",
       "      <th>WordPercentile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13297</th>\n",
       "      <td>6612</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1222957370302697472</td>\n",
       "      <td>virus</td>\n",
       "      <td>1222957370302697472</td>\n",
       "      <td>4502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>6612</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>1222957370302697472</td>\n",
       "      <td>mdr</td>\n",
       "      <td>1222957370302697472</td>\n",
       "      <td>4502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13299</th>\n",
       "      <td>6612</td>\n",
       "      <td>3</td>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>1222957370302697472</td>\n",
       "      <td>raciste</td>\n",
       "      <td>1222957370302697472</td>\n",
       "      <td>4502</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>6612</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1223557159285337856</td>\n",
       "      <td>lycéens</td>\n",
       "      <td>1223557159285337856</td>\n",
       "      <td>3194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7952</th>\n",
       "      <td>6612</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>1223557159285337856</td>\n",
       "      <td>place</td>\n",
       "      <td>1223557159285337856</td>\n",
       "      <td>3194</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          D  Rank    f  idf             tweetid     word       AUTHORTWEETID  \\\n",
       "13297  6612     1   14    6 1222957370302697472    virus 1222957370302697472   \n",
       "13298  6612     2   61    5 1222957370302697472      mdr 1222957370302697472   \n",
       "13299  6612     3  147    4 1222957370302697472  raciste 1222957370302697472   \n",
       "7951   6612     1   12    6 1223557159285337856  lycéens 1223557159285337856   \n",
       "7952   6612     2   24    6 1223557159285337856    place 1223557159285337856   \n",
       "\n",
       "          w  WordPercentile  \n",
       "13297  4502               0  \n",
       "13298  4502               0  \n",
       "13299  4502               1  \n",
       "7951   3194               0  \n",
       "7952   3194               0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tfidf.merge(TweetsStats,left_on=\"tweetid\",right_on=\"AUTHORTWEETID\")\n",
    "tfidf.sort_values(by=[\"w\",\"tweetid\"],inplace=True,ascending=False)\n",
    "tfidf[\"WordPercentile\"] = tfidf.f.rank(pct=True)\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allah</td>\n",
       "      <td>54151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>islam</td>\n",
       "      <td>40581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vient</td>\n",
       "      <td>38692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raciste</td>\n",
       "      <td>34263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdr</td>\n",
       "      <td>31873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  power\n",
       "0    allah  54151\n",
       "1    islam  40581\n",
       "2    vient  38692\n",
       "3  raciste  34263\n",
       "4      mdr  31873"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[\"power\"] = tfidf.w * tfidf.idf\n",
    "wordcloud = tfidf.groupby(\"word\")[\"power\"].sum().reset_index()\n",
    "wordcloud.sort_values(by=\"power\",ascending=False,inplace=True)\n",
    "wordcloud.reset_index(drop=True,inplace=True)\n",
    "wordcloud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mila</td>\n",
       "      <td>19143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  power\n",
       "22  mila  19143"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcloud[wordcloud.word==\"mila\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tweet analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytweetid = np.random.choice(a = tfidf.tweetid.unique())\n",
    "content,tokens,details = ExploreTweetTreatment(mytweetid,reffam,tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AsSalam a3leykoum wa RahmatouLlahi wa Baarakatouh\n",
      "\n",
      "J’aimerai que toutes les femmes voilées qui lisent ce tweet vien… https://t.co/eLbYDtxmVZ\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assalam', 'a3leykoum', 'rahmatoullahi', 'baarakatouh', 'aimerai', 'femmes', 'voilées', 'lisent', 'tweet', 'vien', 'https', 'elbydtxmvz']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'voilées',\n",
       "  'idf': 6.717199917261079,\n",
       "  'Rank': 1,\n",
       "  'f': 8,\n",
       "  'w': 14,\n",
       "  'WordPercentile': 0.0062179442141502145},\n",
       " {'word': 'tweet',\n",
       "  'idf': 4.622254189045278,\n",
       "  'Rank': 2,\n",
       "  'f': 65,\n",
       "  'w': 14,\n",
       "  'WordPercentile': 0.3962622075458442},\n",
       " {'word': 'femmes',\n",
       "  'idf': 3.906292330719161,\n",
       "  'Rank': 3,\n",
       "  'f': 133,\n",
       "  'w': 14,\n",
       "  'WordPercentile': 0.717136232698658}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExploreTweetTreatment(mytweetid,reffam,tfidf):\n",
    "    content = reffam.AUTHORTWEETCONTENT[reffam.AUTHORTWEETID==mytweetid].iloc[0]\n",
    "    tokens = Tokenizer(content)\n",
    "    details = tfidf[tfidf.tweetid==mytweetid][[\"word\",\"idf\",\"Rank\",\"f\",\"w\",\"WordPercentile\"]].\\\n",
    "    to_dict(orient=\"records\")\n",
    "    return content,tokens,details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \n",
    "    # Initialisation de l'objet\n",
    "    def __init__(self,RemoveWordsPeriod,TooFrequentThreshold,TooInfrequentThreshold):\n",
    "        \n",
    "        self.WORDS2TWEETS = pd.DataFrame()\n",
    "        self.DOCSTOUCHED = pd.DataFrame()\n",
    "        self.COMPTEUR = {}\n",
    "        self.BATCH_WORDS2TWEETS = pd.DataFrame()\n",
    "        self.BATCH_DOCSTOUCHED = pd.DataFrame()\n",
    "        self.BATCH_COMPTEUR = {}\n",
    "        self.BATCH_V = []\n",
    "        self.DocsRepresentation = pd.DataFrame()\n",
    "        self.RemoveWordsPeriod = RemoveWordsPeriod\n",
    "        self.TooFrequentThreshold = TooFrequentThreshold\n",
    "        self.TooInfrequentThreshold = TooInfrequentThreshold\n",
    "        \n",
    "        self.LastDate = 0\n",
    "     \n",
    "    \n",
    "    def LoadData(self):\n",
    "        \n",
    "        self.WORDS2TWEETS = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_words2tweets.pkl\"))\n",
    "        self.DOCSTOUCHED = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_docstouched.pkl\"))\n",
    "        self.COMPTEUR = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_compteur.pkl\"),typeobj=\"dic\")\n",
    "        self.DocsRepresentation = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_DocsRepresentation.pkl\"))\n",
    "        self.LastDate = LoadPickleOrInit(os.path.join(Root,FolderProject,\"tfidf_lastdate.pkl\"),typeobj=\"0\")\n",
    "        \n",
    "        if len(self.DOCSTOUCHED)==0:\n",
    "            print(\"No data!\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def SaveOnDisk(self):\n",
    "        \n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_compteur.pkl\"),self.COMPTEUR)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_docstouched.pkl\"),self.DOCSTOUCHED)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_words2tweets.pkl\"),self.WORDS2TWEETS)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_DocsRepresentation.pkl\"),self.DocsRepresentation)\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"tfidf_lastdate.pkl\"),self.LastDate)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    # EVALUATION de la variable globale |D|\n",
    "    # i.e. le nombre de documents, i.e. les tweets\n",
    "    def EvaluateD(self):\n",
    "        if len(self.WORDS2TWEETS)>0:\n",
    "            res = len(np.unique(self.WORDS2TWEETS.tweetid))\n",
    "        else:\n",
    "            res = 0\n",
    "        return res\n",
    "    \n",
    "    # CREATION de la liste de batch\n",
    "    def ProcessCorpus(self,TweetsDataFrame):\n",
    "        \n",
    "        tweetsdf = TweetsDataFrame.copy()\n",
    "        \n",
    "        if len(self.WORDS2TWEETS)>0:\n",
    "            a = self.WORDS2TWEETS.tweetid.unique()\n",
    "            b = tweetsdf.TWEETID\n",
    "            fil = ~b.isin(a)\n",
    "            tweetsdf = tweetsdf[fil]\n",
    "            \n",
    "        tweetsdf = tweetsdf.copy()[tweetsdf.AUTHORTWEETUNIXEPOCH>self.LastDate]\n",
    "        ListOfTweetsDF = SplitTweetsToAnalyse(tweetsdf,self.RemoveWordsPeriod)\n",
    "        self.WaitingCorpusList = ListOfTweetsDF\n",
    "        \n",
    "        if len(ListOfTweetsDF)>0:\n",
    "            self.LastDate = self.WaitingCorpusList[-1].AUTHORTWEETUNIXEPOCH.max()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "    # TRAITEMENT DE LA LISTE DE BATCH\n",
    "    def ComputeCorpus(self):\n",
    "        for iddf,df in enumerate(self.WaitingCorpusList):\n",
    "            print(str(iddf+1),\"/\",str(len(self.WaitingCorpusList)))\n",
    "            self.AddCorpus(df)\n",
    "            self.BuildDocsRepresentation()\n",
    "        return None\n",
    "            \n",
    "            \n",
    "    # TRAITEMENT D'UN BATCH\n",
    "    def AddCorpus(self,tweetsdf):     \n",
    "        \n",
    "        if len(tweetsdf)>0:\n",
    "            \n",
    "            # Build les batch df\n",
    "            tweetsdf.TWEETCONTENT = tweetsdf.TWEETCONTENT.map(lambda a : Tokenizer(a))\n",
    "            Words2TweetsDF = DeplyrDF(tweetsdf)\n",
    "            CompteurDic = BooleanCorpusCompteur(tweetsdf.TWEETCONTENT.tolist())\n",
    "            V = list(CompteurDic.keys())\n",
    "            DocsTouched = BuildDocumentsTouched(V,tweetsdf.TWEETCONTENT.tolist())\n",
    "            DocsTouched = pd.DataFrame(data=DocsTouched,index=[0]).T.reset_index().rename(columns = {\"index\":\"word\",0:\"f\"})\n",
    "            \n",
    "            # Remove les mots trop peu fréquents du batch\n",
    "            TooFrequentRaw = int(self.TooFrequentThreshold * len(Words2TweetsDF.tweetid.unique()))\n",
    "            TooInfrequentRaw = int(self.TooInfrequentThreshold * len(Words2TweetsDF.tweetid.unique()))\n",
    "            DocsTouched = DocsTouched.copy()[(DocsTouched.f>TooInfrequentRaw) & (DocsTouched.f<TooFrequentRaw)]\n",
    "            Words2TweetsDF = Words2TweetsDF.merge(DocsTouched,on=\"word\")\n",
    "            Words2TweetsDF = Words2TweetsDF[[\"word\",\"tweetid\"]]\n",
    "            V = DocsTouched.word.tolist()\n",
    "            \n",
    "            # Assignation des batch df\n",
    "            self.BATCH_WORDS2TWEETS = Words2TweetsDF\n",
    "            self.BATCH_DOCSTOUCHED = DocsTouched\n",
    "            self.BATCH_COMPTEUR = CompteurDic\n",
    "            self.BATCH_V = V\n",
    "\n",
    "            \n",
    "                                                                 \n",
    "                                                                 \n",
    "            #  * * * UPDATING PART * * * \n",
    "            #  * * * UPDATING PART * * *\n",
    "            #  * * * UPDATING PART * * *\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # maj du dic\n",
    "            self.COMPTEUR = UpdateCompteurDic(self.COMPTEUR,self.BATCH_COMPTEUR)\n",
    "            \n",
    "            # maj du words2tweets\n",
    "            self.WORDS2TWEETS = pd.concat((self.WORDS2TWEETS,self.BATCH_WORDS2TWEETS),axis=0)\n",
    "            \n",
    "            # maj du docstouched\n",
    "            if len(self.DOCSTOUCHED)>0:\n",
    "                fil = self.DOCSTOUCHED.word.isin(pd.Series(V))\n",
    "                nepastoucher = self.DOCSTOUCHED[~fil]\n",
    "                amodifier = self.DOCSTOUCHED[fil]\n",
    "                amodifier = pd.concat((amodifier,self.BATCH_DOCSTOUCHED),axis = 0)\n",
    "                amodifier = amodifier.groupby(\"word\")[\"f\"].sum().reset_index()\n",
    "                self.DOCSTOUCHED = pd.concat((amodifier,nepastoucher),axis = 0)\n",
    "            else:\n",
    "                self.DOCSTOUCHED = self.BATCH_DOCSTOUCHED\n",
    "                \n",
    "\n",
    "            \n",
    "        # si tweetsdf est vide alors les batch sont vides\n",
    "        else:\n",
    "            self.BATCH_WORDS2TWEETS = pd.DataFrame()\n",
    "            self.BATCH_DOCSTOUCHED = pd.DataFrame()\n",
    "            self.BATCH_COMPTEUR = {}\n",
    "            self.BATCH_V = []\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def BuildDocsRepresentation(self):\n",
    "        \n",
    "        # Si y'a rien dans le batch alors on s'arrête là\n",
    "        if len(self.BATCH_V)==0:\n",
    "            return None\n",
    "        \n",
    "        # Si y'a déjà des idf scores de calculé on doit la mettre à jour\n",
    "        # Le keepdf, contient les mots non présents dans le batch\n",
    "        if len(self.DocsRepresentation)>0:\n",
    "            fil = self.DocsRepresentation.word.isin(pd.Series(self.BATCH_V))\n",
    "            DocsRepresentationKeep = self.DocsRepresentation.copy()[~fil]\n",
    "            DocsRepresentationKeep[\"D\"] = self.EvaluateD()\n",
    "        else:\n",
    "            DocsRepresentationKeep = pd.DataFrame()\n",
    "        \n",
    "        # df intermediaires\n",
    "        fil = self.DOCSTOUCHED.word.isin(pd.Series(self.BATCH_V))\n",
    "        temptouched = self.DOCSTOUCHED.copy()[fil]\n",
    "        fil = self.WORDS2TWEETS.word.isin(pd.Series(self.BATCH_V))\n",
    "        tempwords2 = self.WORDS2TWEETS[fil]        \n",
    "        toadd = temptouched.merge(tempwords2,on=\"word\")\n",
    "        toadd[\"D\"] = self.EvaluateD()   \n",
    "        \n",
    "        # concaténation des scores des anciens mots et des nouveaux mots\n",
    "        solution = pd.concat((toadd,DocsRepresentationKeep),axis = 0, sort = True)    \n",
    "        solution[\"idf\"] = (solution.D / solution.f).map(lambda a : math.log(a))\n",
    "        solution = solution\n",
    "        solution.reset_index(drop = True,inplace = True)\n",
    "        solution.sort_values(by = [\"tweetid\",\"idf\"],ascending=False,inplace=True)\n",
    "        solution.drop_duplicates(inplace = True)\n",
    "        solution['Rank'] = solution.groupby([\"tweetid\"]).cumcount()+1\n",
    "        self.DocsRepresentation = solution\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(randomstring,french_stopwords=french_stopwords):\n",
    "    translator = str.maketrans(string.punctuation,' '*32)\n",
    "    randomstring = randomstring.translate(translator)\n",
    "    randomstring = randomstring.replace(\"’\",\" \")\n",
    "    randomstring = randomstring.replace(\"`\",\" \")\n",
    "    randomstring = randomstring.replace(\"'\",\" \")\n",
    "    randomstring = randomstring.replace(\"“\",\" \")\n",
    "    randomstring = randomstring.replace(\"”\",\" \")\n",
    "    randomstring = randomstring.replace(\"…\",\" \")\n",
    "    randomstring = randomstring.lower()\n",
    "    randomstring = \" \".join(randomstring.split())\n",
    "    words = randomstring.split(\" \")\n",
    "    words = [item for item in words if item not in french_stopwords]\n",
    "    words = [item for item in words if len(item)>2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceCompteur(compteur,words,weight=1):\n",
    "    for w in words:\n",
    "        compteur[w] = compteur.get(w,0) + weight\n",
    "    return compteur\n",
    "\n",
    "def BooleanCorpusCompteur(Corpus):\n",
    "    compteur = {}\n",
    "    for doc in Corpus:\n",
    "        compteur = SentenceCompteur(compteur,doc)\n",
    "    return compteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDocumentsTouched(V,CleanedCorpus):\n",
    "    \n",
    "    DocumentsTouched = {}\n",
    "    for w in V:\n",
    "        for d in CleanedCorpus:\n",
    "            if w in d:\n",
    "                DocumentsTouched[w] = DocumentsTouched.get(w,0) + 1        \n",
    "    \n",
    "    return DocumentsTouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateCompteurDic(OriginalDic,AddDic):\n",
    "    for k,v in AddDic.items():\n",
    "        OriginalDic[k] = OriginalDic.get(k,0) + v\n",
    "    return OriginalDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeplyrDF(TweetsDataFrame):\n",
    "\n",
    "    L = []\n",
    "    for i,row in TweetsDataFrame.iterrows():\n",
    "        tweetid = row[\"TWEETID\"]\n",
    "        tweetcontent = row[\"TWEETCONTENT\"]\n",
    "        tweetcontent = pd.Series(tweetcontent)\n",
    "        tempdf = tweetcontent.to_frame(name = \"word\")\n",
    "        tempdf[\"tweetid\"] = tweetid\n",
    "        L.append(tempdf)\n",
    "\n",
    "    res = pd.concat(L,axis=0)\n",
    "    res.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitTweetsToAnalyse(TweetsToAnalyse,RemoveWordsPeriod):\n",
    "    TweetsToAnalyse[\"TimeElapsed\"] = (TweetsToAnalyse.AUTHORTWEETUNIXEPOCH.max() - TweetsToAnalyse.AUTHORTWEETUNIXEPOCH) / RemoveWordsPeriod\n",
    "    TweetsToAnalyse[\"TimeElapsed\"] = TweetsToAnalyse[\"TimeElapsed\"].astype(int)\n",
    "    idgroup = TweetsToAnalyse.TimeElapsed.unique()\n",
    "    idgroup.sort()\n",
    "    L = []\n",
    "    for idg in idgroup[:-1]:\n",
    "        tempdf = TweetsToAnalyse.copy()[TweetsToAnalyse.TimeElapsed==idg]\n",
    "        L.append(tempdf)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
