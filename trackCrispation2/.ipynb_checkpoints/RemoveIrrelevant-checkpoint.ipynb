{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "pd.options.display.float_format = '{:.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadJsonFile(filename): \n",
    "    with open(filename, 'r') as f:\n",
    "        DicConfig = json.load(f)\n",
    "    return DicConfig\n",
    "\n",
    "\n",
    "def GlobalDicDeplier(OneDic):\n",
    "    for k,v in OneDic.items():\n",
    "        exec('globals()[k] = v')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Config variables\n"
     ]
    }
   ],
   "source": [
    "DicConfig = LoadJsonFile(os.path.join(os.getcwd(),\"config.json\"))\n",
    "GlobalDicDeplier(DicConfig)\n",
    "sys.path.append(Root)\n",
    "from fun import *\n",
    "\n",
    "print(\"Load Config variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(Root,FolderProject,\"RefFam.pkl\")\n",
    "RefFam = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"RefRT.pkl\")\n",
    "RefRT = LoadPickleOrInit(path)\n",
    "\n",
    "path = os.path.join(Root,FolderProject,\"ISdf.pkl\")\n",
    "OldISdf = LoadPickleOrInit(path)\n",
    "\n",
    "print(\"Load Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstBassineID = ExtractFirstBID(RefRT,OldISdf,bassine_size)\n",
    "datelastaction = RefRT.TWEETUNIXEPOCH.max()\n",
    "print(\"Define Metaparameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractQualifiedRT(RefFam,bassine_size,FirstBassineID):\n",
    "\n",
    "    df = RefRT.copy()\n",
    "\n",
    "    BassineID = RefFam.AUTHORTWEETUNIXEPOCH/bassine_size\n",
    "    RefFam[\"BassineID\"] = BassineID.astype(int)\n",
    "    RefFam[\"BassineSize\"] = bassine_size\n",
    "    RefFam = RefFam[RefFam.BassineID>=FirstBassineID]\n",
    "    RefFam.reset_index(drop=False,inplace=True)\n",
    "\n",
    "    df = df[df.AUTHORTWEETID.isin(RefFam.AUTHORTWEETID)]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df = df.merge(RefFam[[\"AUTHORTWEETID\",\"AUTHORTWEETUNIXEPOCH\"]],on=\"AUTHORTWEETID\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddSomeStatsRT(df,bassine_size,bassine_recul,FirstBassineID):\n",
    "\n",
    "    BassineID = df.AUTHORTWEETUNIXEPOCH/bassine_size\n",
    "    df[\"BassineID\"] = BassineID.astype(int)\n",
    "    df[\"BassineSize\"] = bassine_size\n",
    "    df[\"age\"] = df.TWEETUNIXEPOCH - df.AUTHORTWEETUNIXEPOCH\n",
    "    df[\"recul\"] = datelastaction - df.AUTHORTWEETUNIXEPOCH\n",
    "\n",
    "    minreculdf = df.groupby(\"BassineID\")[\"recul\"].min().reset_index()\n",
    "    minreculdf = minreculdf[minreculdf.recul>=bassine_recul]\n",
    "    minreculdf = minreculdf[minreculdf.BassineID>=FirstBassineID]\n",
    "    minreculdf = minreculdf.rename(columns = {\"recul\":\"minrecul\"}).reset_index(drop = True)\n",
    "    \n",
    "    df = df.merge(minreculdf,on=\"BassineID\")\n",
    "    df = df[df.age<=bassine_recul]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildStatsIrrelevant(tempdf,bassine_threshold,bassine_size):\n",
    "\n",
    "    y = tempdf.groupby([\"BassineID\",\"AUTHORTWEETID\"]).size().reset_index().rename(columns = {0:\"f\"})\n",
    "    r = y.groupby('BassineID')['f'].rank(ascending=False).reset_index(drop=True)\n",
    "    y[\"Rank\"] = r\n",
    "    y.sort_values(by=[\"BassineID\",\"f\"],ascending=False,inplace=True)\n",
    "    y.reset_index(drop = True,inplace = True)\n",
    "    y[\"cumsumf\"] = y.groupby(\"BassineID\")[\"f\"].apply(lambda x: x.cumsum())\n",
    "    sumdf = y.groupby(\"BassineID\")[\"f\"].sum().reset_index().rename(columns = {\"f\":\"sum\"})\n",
    "    y = y.merge(sumdf,on=\"BassineID\")\n",
    "    y[\"p\"] = (100*y.cumsumf) / y[\"sum\"]\n",
    "    y = y[y.p<bassine_threshold]\n",
    "    y[\"BassineSize\"]  = bassine_size\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateISdf(RefFam,bassine_size,FirstBassineID,bassine_recul,bassine_threshold,Root,FolderProject):\n",
    "    \n",
    "    global NQualifiedRT\n",
    "    global NAddSomeStatsRT\n",
    "    global NISdf\n",
    "    global NISdfFinal\n",
    "    \n",
    "    NQualifiedRT=0\n",
    "    NAddSomeStatsRT=0\n",
    "    NISdf=0\n",
    "    NISdfFinal=0\n",
    "    \n",
    "    print(\"Extracting Qualified RT\")\n",
    "    df = ExtractQualifiedRT(RefFam,bassine_size,FirstBassineID)\n",
    "    NQualifiedRT = len(df)\n",
    "    print(\"rows : \",str(NQualifiedRT))\n",
    "    \n",
    "    if len(df)>0:\n",
    "        print(\"Add Some Stats to Qualified RT\")\n",
    "        df = AddSomeStatsRT(df,bassine_size,bassine_recul,FirstBassineID)\n",
    "        NAddSomeStatsRT = len(df)\n",
    "        print(\"rows : \",str(NAddSomeStatsRT))\n",
    "        \n",
    "    else:\n",
    "        print(\"Not enough data\")\n",
    "        NAddSomeStatsRT = 0\n",
    "        return None\n",
    "    \n",
    "    if len(df)>0:\n",
    "        print(\"Compute Stats for Relevant RT\")\n",
    "        ISdf = BuildStatsIrrelevant(df,bassine_threshold,bassine_size)\n",
    "        NISdf = len(ISdf)\n",
    "        print(\"Stack new data to OldISdf\")\n",
    "        ISdf = OldISdf.append(ISdf,ignore_index=True)\n",
    "        ISdf.reset_index(drop=True,inplace=True)\n",
    "        NISdfFinal = len(ISdf)\n",
    "        print(\"Save ISdf.pkl on disk\")\n",
    "        PickleDump(os.path.join(Root,FolderProject,\"ISdf.pkl\"),ISdf)\n",
    "        print(str(NISdf),\" ont été rajoutées\")\n",
    "        print(str(NISdfFinal),\" au total dans le fichier ISdf.pkl\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Not enough data\")\n",
    "        NISdf = 0\n",
    "        NISdfFinal = len(OldISdf)\n",
    "        return None\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Update ISdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Qualified RT\n",
      "rows :  1474\n",
      "Add Some Stats to Qualified RT\n",
      "rows :  599\n",
      "Compute Stats for Relevant RT\n",
      "Stack new data to OldISdf\n",
      "Save ISdf.pkl on disk\n",
      "30  ont été rajoutées\n",
      "101  au total dans le fichier ISdf.pkl\n"
     ]
    }
   ],
   "source": [
    "UpdateISdf(RefFam,\n",
    "           bassine_size,\n",
    "           FirstBassineID,\n",
    "           bassine_recul,\n",
    "           bassine_threshold,\n",
    "           Root,\n",
    "           FolderProject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "RelevantLogs = {\"DateRun\" : GetCurrentTime(),\n",
    "         \"FirstBassineID\" : FirstBassineID,\n",
    "         \"DateLastAction\" : str(pd.to_datetime(datelastaction,unit=\"s\")),\n",
    "         \"NQualifiedRTA\" :NQualifiedRT, \n",
    "         \"NQualifiedRTB\" :NAddSomeStatsRT,\n",
    "         \"NRelevantTweets\" : NISdf,\n",
    "         \"TotalNRelevantTweets\" : NISdfFinal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write logs\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(Root,FolderProject,\"Relevant.log\")\n",
    "AppendStringToFile(filename,RelevantLogs)\n",
    "\n",
    "print(\"Write logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Extracting Qualified RT\")\n",
    "df = ExtractQualifiedRT(RefFam,bassine_size,FirstBassineID)\n",
    "print(\"rows : \",str(len(df)))\n",
    "\n",
    "print(\"Add Some Stats to Qualified RT\")\n",
    "df = AddSomeStatsRT(df,bassine_size,bassine_recul,FirstBassineID)\n",
    "print(\"rows : \",str(len(df)))\n",
    "\n",
    "print(\"Compute Stats for Relevant RT\")\n",
    "ISdf = BuildStatsIrrelevant(df,bassine_threshold,bassine_size)\n",
    "addrow = len(ISdf)\n",
    "print(\"Stack new data to OldISdf\")\n",
    "\n",
    "ISdf = OldISdf.append(ISdf,ignore_index=True)\n",
    "ISdf.reset_index(drop=True,inplace=True)\n",
    "finalrow = len(ISdf)\n",
    "print(\"Save ISdf.pkl on disk\")\n",
    "\n",
    "PickleDump(os.path.join(Root,FolderProject,\"ISdf.pkl\"),ISdf)\n",
    "print(str(addrow),\" ont été rajoutées\")\n",
    "print(str(finalrow),\" au total dans le fichier ISdf.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
