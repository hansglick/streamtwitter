{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "french_stopwords = list(fr_stop)\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words\n",
    "Fonction afin de compter les mots uniques dans une liste de documents pondérés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "doca = \"je vais au tennis ce  matin a 14h\"\n",
    "docb = \"je vais à la cantine à midi\"\n",
    "DocsWeights = [(doca,4),(docb,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpandWeightedDocs(DocsWeights):\n",
    "    \n",
    "    L = []\n",
    "    for d,w in DocsWeights:\n",
    "        L = L + [d]*w\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(randomstring,french_stopwords=french_stopwords):\n",
    "    randomstring = \" \".join(randomstring.split())\n",
    "    words = randomstring.split(\" \")\n",
    "    words = [item for item in words if item not in french_stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentenceCompteur(compteur,words,weight=1):\n",
    "    for w in words:\n",
    "        compteur[w] = compteur.get(w,0) + weight\n",
    "    return compteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractCompteur(DocsWeights):\n",
    "    compteur = {}\n",
    "    for doc,coefficient in DocsWeights:\n",
    "        sentence = Tokenizer(doc)\n",
    "        compteur = SentenceCompteur(compteur,sentence,coefficient)\n",
    "    return compteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "compteur = ExtractCompteur(DocsWeights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Time Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefineTimeMarks(lastdate,firstdate,step,window):\n",
    "\n",
    "    mark0 = firstdate + window\n",
    "    start = mark0\n",
    "    L = [start]\n",
    "\n",
    "    while(True):\n",
    "        newmark = start+step\n",
    "        if newmark>lastdate:\n",
    "            break\n",
    "        else:\n",
    "            L.append(newmark)\n",
    "            start=newmark\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastdate = 1000\n",
    "firstdate = 587\n",
    "step = 50\n",
    "window = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean RefRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastTimeMarkTreated = 14555\n",
    "LastTimeMarkTreated - window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets of Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomtm = TimeMarksList[-1]\n",
    "TweetsAuthorsDF = BuildTweetsAuthorFromTimeMark(randomtm,WindowSize,StepSize,rtdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL IN ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "# ...\n",
    "OldListOfTMResults = PickleLoad(os.path.join(Root,FolderProject,\"SequenceOfGraphResults.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE PREREQUISITE VARIABLES\n",
    "firstdate,lastdate = DefineFirstLastDate(rtdf,StepSize)\n",
    "ListOfTM = DefineTimeMarks(lastdate,firstdate,StepSize,WindowSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYTICS\n",
    "ListOfTMResults = ListOfTMTreatment(ListOfTM,WindowSize,StepSize,rtdf)\n",
    "LogTMAddSize = len(ListOfTMResults)\n",
    "LogBasicStats = [(len(item[\"Links\"]),len(item[\"Tweets\"])) for item in ListOfTMResults]\n",
    "LogBasicStats = np.array(LogBasicStats)\n",
    "LogBasicStats = stats.describe(LogBasicStats)\n",
    "LogBasicStats = dict(LogBasicStats._asdict())\n",
    "\n",
    "ListOfTMResults = ListOfTMResults + OldListOfTMResults\n",
    "LogTMNowSize = len(ListOfTMResults)\n",
    "LogBasicStats2 = [(len(item[\"Links\"]),len(item[\"Tweets\"])) for item in ListOfTMResults]\n",
    "LogBasicStats2 = np.array(LogBasicStats2)\n",
    "LogBasicStats2 = stats.describe(LogBasicStats2)\n",
    "LogBasicStats2 = dict(LogBasicStats2._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE RESULTS\n",
    "PickleDump(os.path.join(Root,FolderProject,\"SequenceOfGraphResults.pkl\"),ListOfTMResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE LOGS\n",
    "mylog = {\"TMAddSize\":LogTMAddSize,\n",
    "         \"TMNowSize\":LogTMNowSize,\n",
    "         \"AddBasicStats\":LogBasicStats,\n",
    "         \"NewBasicStats\":LogBasicStats2}\n",
    "WriteLogs(os.path.join(Root,FolderProject,\"Graph.log\"),mylog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefineFirstLastDate(rtdf,StepSize):\n",
    "\n",
    "    lastdate = rtdf.TWEETUNIXEPOCH.max()\n",
    "\n",
    "    if len(OldListOfTMResults) == 0:\n",
    "        firstdate = rtdf.TWEETUNIXEPOCH.min()\n",
    "    else:\n",
    "        LastTimeMark = max([item[\"TimeMark\"] for item in OldListOfTMResults])\n",
    "        firstdate = LastTimeMark + StepSize\n",
    "\n",
    "    return firstdate,lastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListOfTMTreatment(ListOfTM,WindowSize,StepSize,rtdf):\n",
    "\n",
    "    L = []\n",
    "    for randomtm in ListOfTM:\n",
    "        TimeMarkResultsDic = {}\n",
    "        LinksDic = BuildLinksDocFromTimeMark(randomtm,WindowSize,StepSize,rtdf)\n",
    "        TweetsAuthorsDF = BuildTweetsAuthorFromTimeMark(randomtm,WindowSize,StepSize,rtdf)\n",
    "        TimeMarkResultsDic[randomtm] = {\"Links\":LinksDic,\n",
    "                                        \"Tweets\":TweetsAuthorsDF,\n",
    "                                        \"TimeMark\":randomtm,\n",
    "                                        \"LinksWindow\":randomtm - WindowSize,\n",
    "                                        \"TweetsWindow\":randomtm - StepSize}\n",
    "        L.append(TimeMarkResultsDic)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildTweetsAuthorFromTimeMark(randomtm,WindowSize,StepSize,rtdf):\n",
    "    \n",
    "    periods = ExtractTheTwoPeriods(randomtm,WindowSize,StepSize)\n",
    "    debut,fin = periods[\"TweetWindow\"]\n",
    "    myfilter = (rtdf.TWEETUNIXEPOCH>=debut) & (rtdf.TWEETUNIXEPOCH<=fin)\n",
    "    temprtdf = rtdf.copy()[myfilter]\n",
    "    temprtdf.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    # Modifier authoridname et tweetidname\n",
    "    #TweetsAuthorsDF = temprtdf.groupby(authoridname)[tweetidname].apply(list).reset_index()\n",
    "    TweetsAuthorsDF = temprtdf.groupby([\"AUTHORID\",\"AUTHORTWEETID\"]).size().reset_index()\n",
    "    \n",
    "    \n",
    "    return TweetsAuthorsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean RTDF, another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On charge la liste de results\n",
    "2. On trouve la derniere fenetre\n",
    "3. On extrait la borne supérieure de la fenetre\n",
    "4. On supprime tout les rt dont la date d'émission est inférieur à la borne supérieur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "# ... rtdf\n",
    "ListOfTMResults = PickleLoad(os.path.join(Root,FolderProject,\"SequenceOfGraphResults.pkl\"))\n",
    "LastTimestamp = max([item[\"TimeMark\"] for item in ListOfTMResults ])\n",
    "FirstTimestamp = min([item[\"TimeMark\"] for item in ListOfTMResults ])\n",
    "CleanRTDF = rtdf.copy()[rtdf.TWEETUNIXEPOCH>LastTimestamp]\n",
    "PickleDump(os.path.join(Root,FolderProject,\"rtdf.pkl\"),CleanRTDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de lignes dans rtdf avant / apres\n",
    "LogNoldRTDF = len(rtdf)\n",
    "LogNnewRTDF = len(CleanRTDF)\n",
    "\n",
    "LogFirstDateOld = str(pd.to_datetime(rtdf.TWEETUNIXEPOCH.min(),unit=\"s\"))\n",
    "LogLastDateOld = str(pd.to_datetime(rtdf.TWEETUNIXEPOCH.max(),unit=\"s\"))\n",
    "\n",
    "LogFirstDateNew = str(pd.to_datetime(CleanRTDF.TWEETUNIXEPOCH.min(),unit=\"s\"))\n",
    "LogLastDateNew = str(pd.to_datetime(CleanRTDF.TWEETUNIXEPOCH.max(),unit=\"s\"))\n",
    "\n",
    "log ={\"NoldRTDF\":LogNoldRTDF,\n",
    "      \"NnewRTDF\":LogNnewRTDF,\n",
    "      \"FirstDateOld\":LogFirstDateOld,\n",
    "      \"LastDateOld\":LogLastDateOld,\n",
    "      \"FirstDateNew\":LogFirstDateNew,\n",
    "      \"LastDateNew\":LogLastDateNew}\n",
    "\n",
    "WriteLogs(os.path.join(Root,FolderProject,\"Clean.log\"),log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque cluster : \n",
    " * Sélection des lignes appartenant à un cluster\n",
    " * On extrait tout les tweets ids des nodes\n",
    " * "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
